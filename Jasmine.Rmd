---
title: "R Notebook"
output: html_notebook
---


## Load Data
```{r}
## Loading the data from Dropbox/MGTA455-2019/data/
pentathlon_nptb <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/pentathlon_nptb.rds"))
```


```{r}
## filter and sort the dataset
pentathlon_nptb_train <- pentathlon_nptb %>%
  filter(training == 1) %>%
  select(custid:representative)
register("pentathlon_nptb_train", "pentathlon_nptb")

# dtab(pentathlon_nptb_train, dec = 2, nr = 100) %>% render()
pentathlon_nptb_test <- pentathlon_nptb %>%
  filter(training == 0) %>%
  select(custid:representative)
register("pentathlon_nptb_test", "pentathlon_nptb")
```


## Create a function to calculate auc and accuracy.
```{r}
cm <- function(dat, vars){
  
  cm_df <- as.data.frame(matrix(NA, ncol = 3, nrow = length(vars)))
  colnames(cm_df) <- c("var", "auc", "accuracy")
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    probs <- pull(dat, !!var)
    resp <- pull(dat, "buyer")
    
    predict <- ifelse(pull(dat, !!var) > 0.5, "TRUE", "FALSE") # predict whether a customer will buy 
    
    accuracy <- (sum(resp == "yes" & predict == "TRUE") + sum(resp == "no" & predict == "FALSE"))/nrow(dat)
    
    auc <- ModelMetrics::auc(ifelse(resp=="yes",1,0), probs)
    
    cm_vec <- c(var, auc, accuracy)
    cm_df[i,] <- cm_vec
  }
  return(cm_df)
}
```

## Logistic regression with interaction

Logistic 1 : With all best 19 interactions

```{r}
result <- logistic(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"
  ), 
  lev = "yes", 
  int = c(
    "message:age", "message:gender", 
    "age:gender", "age:income", 
    "age:education", "age:children", 
    "age:freq_endurance", 
    "age:freq_strength", 
    "age:freq_water", "age:freq_team", 
    "gender:income", "income:education", 
    "income:children", "income:freq_endurance", 
    "income:freq_strength", 
    "income:freq_water", 
    "income:freq_backcountry", 
    "education:freq_winter", 
    "education:freq_racquet"
  )
)
summary(result)

pred <- predict(result, pred_data = pentathlon_nptb_test)
pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = "pred_logit")
```
Show the evaluation of logistic model.

```{r}
cm(pentathlon_nptb_test,'pred_logit')
```
The auc for the best logistic model is 0.8838.

## Neural Network.

We create grid search for neural network. The parameters we loop through are as follow.
Neural network size = 1,2,3,4,5,6,7,8,9,10
decay = 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1

```{r eval = FALSE}
## create vector storing paremeters. 
size = seq(1,10,1)
decay = seq(0.1,1,0.1)

## create grid search matrix
params <- expand.grid(size,decay)
#params$auc <- NA
colnames(params) <- c("size",'decay')

auc <- c()
accuracy <- c()
for (i in 1:nrow(params)){
 
  result <- nn(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"), 
  lev = "yes", 
  size = params[i,1], 
  decay = params[i,2], 
  seed = 1234
)
  pred <- predict(result, pred_data = pentathlon_nptb_test)
  name_ <- paste0('pred_nn',params[i,1],'_',params[i,2])
  print(name_)
  pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = name_)
  
  
  
  auc_ <- cm(pentathlon_nptb_test,name_)$auc
  accuracy_ <- cm(pentathlon_nptb_test,name_)$accuracy
  auc<- c(auc,auc_)
  accuracy <-c(accuracy,accuracy_)
}
params$auc <- auc
params$accuracy <- accuracy
params <- arrange(params, desc(params$accuracy))
saveRDS(params, "nn_tune.rds")
```

Get the result

```{r} 
nn_tune <-readRDS("nn_tune.rds")
head(nn_tune)

```

The best size is 9 and best decay is 0.8. We then predict based on this model.

```{r}
result <- nn(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"), 
  lev = "yes", 
  size = 9, 
  decay = 0.8, 
  seed = 1234
)
pred <- predict(result, pred_data = pentathlon_nptb_test)
pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = 'pred_nn')
```

## xgboost 

Create dataframe needed for xgboost training.

```{r}
library(caret)
library(xgboost)

pentathlon_nptb_xgb <- pentathlon_nptb

temp <- dummyVars(~message +age ,data = pentathlon_nptb_xgb)
pred <- predict(temp, newdata = pentathlon_nptb_xgb)

pentathlon_nptb_xgb <- pentathlon_nptb_xgb %>%
  cbind(pred) %>%
  select(-c('custid','message','age','total_os','endurance_os','strength_os','water_os','team_os','backcountry_os','winter_os','racquet_os','representative','message.endurance','age.< 30')) %>%
  mutate(buyer = as.factor(buyer),
         gender =ifelse(gender == 'M',1,0))
```


```{r}
pentathlon_nptb_xgb_train <- pentathlon_nptb_xgb %>%
  filter(training == 1) %>%
  select(-training)
  
pentathlon_nptb_xgb_test <- pentathlon_nptb_xgb %>%
  filter(training == 0) %>%
  select(-training)

#### Create xgboost-specific DMatrix

X_train = pentathlon_nptb_xgb_train %>% select(-buyer)
y_train = pentathlon_nptb_xgb_train$buyer

X_test = pentathlon_nptb_xgb_test %>% select(-buyer)
y_test =pentathlon_nptb_xgb_test$buyer

#dtest <- xgb.DMatrix(data = X_test, label = y_test)
#dtrain <- xgb.DMatrix(data = X_train, label = y_train)
```


First, we generate rough parameters to test using manual grid search.

```{r}
pentathlon_nptb_xgb_train2 <- pentathlon_nptb_xgb_train %>%
  mutate(buyer = ifelse(buyer == 'yes',1,0))

pentathlon_nptb_xgb_test2 <- pentathlon_nptb_xgb_test %>%
  mutate(buyer = ifelse(buyer == 'yes',1,0))


X_train2 = model.matrix(buyer~.,pentathlon_nptb_xgb_train2)
#X_train2 <- X_train2[,-2] 
y_train2 = pentathlon_nptb_xgb_train2$buyer

X_test2 = model.matrix(buyer~.,pentathlon_nptb_xgb_test2)
#X_test2 <- X_test2[, -2]
y_test2 = pentathlon_nptb_xgb_test2$buyer


dtest <- xgb.DMatrix(data = X_test2, label = y_test2)
dtrain <- xgb.DMatrix(data = X_train2, label = y_train2)
```

First tune the hyperparameters for the model.

```{r eval = FALSE}
## create vector storing paremeters. 
nrounds = c(20,30)
objective = "binary:logistic"
eta <- c(0.2,0.3)
max_depth <- c(6,7,8)
## create grid search matrix
params <- expand.grid(nrounds,objective,eta,max_depth)
#params$auc <- NA
colnames(params) <- c("nrounds",'objective', "eta",'max_depth')


AUC <- c()
train_auc <-c()

for (i in 1:nrow(params)){
 
  xgb <- xgb.train(
    nrounds = params[i,1],
    params = as.list(params[i,-1]),
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")
  
  auc_ <- tail(xgb$evaluation_log$val_auc,1)
  train_auc_ <- tail(xgb$evaluation_log$train_auc,1)
  AUC<- c(AUC,auc_)
  train_auc <-c(train_auc,train_auc_)
}

params$auc <- AUC
params$train_auc <- train_auc

saveRDS(params, "xgb_tune_manual.rds")

#params <- arrange(params, desc(params$auc))
#saveRDS(params, "xgb_tune_manual.rds")
```

The tuning result is as follows.

```{r}
xgb_tune <-readRDS("xgb_tune_manual.rds")
xgb_tune <- arrange(xgb_tune, desc(xgb_tune$auc))
xgb_tune
```

As we can see from the table, almost all models perform better when nrounds is 30 compared to 20,so we make the prediction that the best nrounds may be larger than 30 and further tuning is needed. The best max_depth is 7 and best eta is 0.2

We then use the elementary result for further and more accurate training using caret package.

```{r eval = FALSE}

auc <- function(data, lev = NULL, model = NULL) {
  c(auc = radiant.model::auc(data$yes, data$obs, 'yes'))
}


xgb_grid_1 = expand.grid(
  nrounds = c(30,50,100),
  eta = c(0.2),
  max_depth = c(7),
  gamma = 1,
  colsample_bytree = 0.8,
  min_child_weight = 10,
  subsample = 0.8
  
)

xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 2,
 
  classProbs = TRUE,
  #returnData = TRUE,
  #returnResamp = "all",                                                                               # set to TRUE for AUC to be computed
  #search = 'grid',
  summaryFunction = auc,
  verboseIter = TRUE
  #allowParallel = FALSE
)

xgb_train_1 = train(
  x = X_train,
  y = y_train,
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  metric = "auc",
  method = "xgbTree"
)

xgb_table <- xgb_train_1$results
saveRDS(xgb_table, "data/xgb_tune_caret.rds")
```


```{r}
readRDS("data/xgb_tune_caret.rds")

```

We found the best nrounds is 30. 

Up to now, we have got all best parameters for xgboost model we need. And we train the final model.
```{r eval = FALSE}
params <- list(
  objective = "binary:logistic",
  eta = 0.2,
  max_depth = 7,
  colsamply_bytree = 0.8,
  min_child_weight = 10,
  subsample = 0.8
  #gamma = 10
)

set.seed(123)
xgb <- xgb.train(
    nrounds = 30,
    params = params,
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")


pred = predict(xgb, dtest)
pentathlon_nptb_test$`pred_xgb` <- pred
#model_eval(intuit75k_test, 'pred_xgb')
AUC <- tail(xgb$evaluation_log$val_auc,1)

saveRDS(pred, "data/xgb_best.rds")
AUC
```

The final auc of the best xgboost model is 0.8871 and the prediction is stored in columnï¼špred_xgb in test set.
