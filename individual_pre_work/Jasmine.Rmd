---
title: "R Notebook"
output: html_notebook
---


## Load Data
```{r}
## Loading the data from Dropbox/MGTA455-2019/data/
pentathlon_nptb <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/pentathlon_nptb.rds"))
```


```{r}
## filter and sort the dataset
pentathlon_nptb_train <- pentathlon_nptb %>%
  filter(training == 1) %>%
  select(custid:representative)
register("pentathlon_nptb_train", "pentathlon_nptb")

# dtab(pentathlon_nptb_train, dec = 2, nr = 100) %>% render()
pentathlon_nptb_test <- pentathlon_nptb %>%
  filter(training == 0) %>%
  select(custid:representative)
register("pentathlon_nptb_test", "pentathlon_nptb")
```


## Create a function to calculate auc and accuracy.
```{r}
cm <- function(dat, vars){
  
  cm_df <- as.data.frame(matrix(NA, ncol = 3, nrow = length(vars)))
  colnames(cm_df) <- c("var", "auc", "accuracy")
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    probs <- pull(dat, !!var)
    resp <- pull(dat, "buyer")
    
    predict <- ifelse(pull(dat, !!var) > 0.5, "TRUE", "FALSE") # predict whether a customer will buy 
    
    accuracy <- (sum(resp == "yes" & predict == "TRUE") + sum(resp == "no" & predict == "FALSE"))/nrow(dat)
    
    auc <- ModelMetrics::auc(ifelse(resp=="yes",1,0), probs)
    
    cm_vec <- c(var, auc, accuracy)
    cm_df[i,] <- cm_vec
  }
  return(cm_df)
}
```

## Logistic regression with interaction

Logistic 1 : With all best 19 interactions

```{r}
result <- logistic(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"
  ), 
  lev = "yes", 
  int = c(
    "message:age", "message:gender", 
    "age:gender", "age:income", 
    "age:education", "age:children", 
    "age:freq_endurance", 
    "age:freq_strength", 
    "age:freq_water", "age:freq_team", 
    "gender:income", "income:education", 
    "income:children", "income:freq_endurance", 
    "income:freq_strength", 
    "income:freq_water", 
    "income:freq_backcountry", 
    "education:freq_winter", 
    "education:freq_racquet"
  )
)
summary(result)

pred <- predict(result, pred_data = pentathlon_nptb_test)
pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = "pred_logit")
```
Show the evaluation of logistic model.

```{r}
cm(pentathlon_nptb_test,'pred_logit')
```
The auc for the best logistic model is 0.8838.

## Neural Network.

We create grid search for neural network. The parameters we loop through are as follow.
Neural network size = 1,2,3,4,5,6,7,8,9,10
decay = 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1

```{r eval = FALSE}
## create vector storing paremeters. 
size = seq(1,10,1)
decay = seq(0.1,1,0.1)

## create grid search matrix
params <- expand.grid(size,decay)
#params$auc <- NA
colnames(params) <- c("size",'decay')

auc <- c()
accuracy <- c()
for (i in 1:nrow(params)){
 
  result <- nn(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"), 
  lev = "yes", 
  size = params[i,1], 
  decay = params[i,2], 
  seed = 1234
)
  pred <- predict(result, pred_data = pentathlon_nptb_test)
  name_ <- paste0('pred_nn',params[i,1],'_',params[i,2])
  print(name_)
  pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = name_)
  
  
  
  auc_ <- cm(pentathlon_nptb_test,name_)$auc
  accuracy_ <- cm(pentathlon_nptb_test,name_)$accuracy
  auc<- c(auc,auc_)
  accuracy <-c(accuracy,accuracy_)
}
params$auc <- auc
params$accuracy <- accuracy
params <- arrange(params, desc(params$accuracy))
saveRDS(params, "nn_tune.rds")
```

Get the result

```{r} 
nn_tune <-readRDS("nn_tune.rds")
nn_tune <- arrange(nn_tune,desc(nn_tune$auc))
head(nn_tune)
```

The best size is 9 and best decay is 0.8. We then predict based on this model.

```{r}
result <- nn(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"), 
  lev = "yes", 
  size = 9, 
  decay = 0.8, 
  seed = 1234
)
pred <- predict(result, pred_data = pentathlon_nptb_test)
pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = 'pred_nn')
```

## xgboost 

Create dataframe needed for xgboost training.

```{r}
library(caret)
library(xgboost)

pentathlon_nptb_xgb <- pentathlon_nptb

temp <- dummyVars(~message +age ,data = pentathlon_nptb_xgb)
pred <- predict(temp, newdata = pentathlon_nptb_xgb)

pentathlon_nptb_xgb <- pentathlon_nptb_xgb %>%
  cbind(pred) %>%
  select(-c('custid','message','age','total_os','endurance_os','strength_os','water_os','team_os','backcountry_os','winter_os','racquet_os','representative','message.endurance','age.< 30')) %>%
  mutate(buyer = as.factor(buyer),
         gender =ifelse(gender == 'M',1,0))
```


```{r}
pentathlon_nptb_xgb_train <- pentathlon_nptb_xgb %>%
  filter(training == 1) %>%
  select(-training)
  
pentathlon_nptb_xgb_test <- pentathlon_nptb_xgb %>%
  filter(training == 0) %>%
  select(-training)

#### Create xgboost-specific DMatrix

X_train = pentathlon_nptb_xgb_train %>% select(-buyer)
y_train = pentathlon_nptb_xgb_train$buyer

X_test = pentathlon_nptb_xgb_test %>% select(-buyer)
y_test =pentathlon_nptb_xgb_test$buyer

#dtest <- xgb.DMatrix(data = X_test, label = y_test)
#dtrain <- xgb.DMatrix(data = X_train, label = y_train)
```


First, we generate rough parameters to test using manual grid search.

```{r}
pentathlon_nptb_xgb_train2 <- pentathlon_nptb_xgb_train %>%
  mutate(buyer = ifelse(buyer == 'yes',1,0))

pentathlon_nptb_xgb_test2 <- pentathlon_nptb_xgb_test %>%
  mutate(buyer = ifelse(buyer == 'yes',1,0))


X_train2 = model.matrix(buyer~.,pentathlon_nptb_xgb_train2)
#X_train2 <- X_train2[,-2] 
y_train2 = pentathlon_nptb_xgb_train2$buyer

X_test2 = model.matrix(buyer~.,pentathlon_nptb_xgb_test2)
#X_test2 <- X_test2[, -2]
y_test2 = pentathlon_nptb_xgb_test2$buyer


dtest <- xgb.DMatrix(data = X_test2, label = y_test2)
dtrain <- xgb.DMatrix(data = X_train2, label = y_train2)
```

First tune the hyperparameters for the model.

```{r eval = FALSE}
## create vector storing paremeters. 
nrounds = c(20,30)
objective = "binary:logistic"
eta <- c(0.2,0.3)
max_depth <- c(6,7,8)
## create grid search matrix
params <- expand.grid(nrounds,objective,eta,max_depth)
#params$auc <- NA
colnames(params) <- c("nrounds",'objective', "eta",'max_depth')


AUC <- c()
train_auc <-c()

for (i in 1:nrow(params)){
 
  xgb <- xgb.train(
    nrounds = params[i,1],
    params = as.list(params[i,-1]),
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")
  
  auc_ <- tail(xgb$evaluation_log$val_auc,1)
  train_auc_ <- tail(xgb$evaluation_log$train_auc,1)
  AUC<- c(AUC,auc_)
  train_auc <-c(train_auc,train_auc_)
}

params$auc <- AUC
params$train_auc <- train_auc

saveRDS(params, "xgb_tune_manual.rds")

#params <- arrange(params, desc(params$auc))
#saveRDS(params, "xgb_tune_manual.rds")
```

The tuning result is as follows.

```{r}
xgb_tune <-readRDS("xgb_tune_manual.rds")
xgb_tune <- arrange(xgb_tune, desc(xgb_tune$auc))
xgb_tune
```

As we can see from the table, almost all models perform better when nrounds is 30 compared to 20,so we make the prediction that the best nrounds may be larger than 30 and further tuning is needed. The best max_depth is 7 and best eta is 0.2

We then use the elementary result for further and more accurate training using caret package.

```{r eval = FALSE}

auc <- function(data, lev = NULL, model = NULL) {
  c(auc = radiant.model::auc(data$yes, data$obs, 'yes'))
}


xgb_grid_1 = expand.grid(
  nrounds = c(30,50,100),
  eta = c(0.2),
  max_depth = c(7),
  gamma = 1,
  colsample_bytree = 0.8,
  min_child_weight = 10,
  subsample = 0.8
  
)

xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 2,
 
  classProbs = TRUE,
  #returnData = TRUE,
  #returnResamp = "all",                                                                               # set to TRUE for AUC to be computed
  #search = 'grid',
  summaryFunction = auc,
  verboseIter = TRUE
  #allowParallel = FALSE
)

xgb_train_1 = train(
  x = X_train,
  y = y_train,
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  metric = "auc",
  method = "xgbTree"
)

xgb_table <- xgb_train_1$results
saveRDS(xgb_table, "data/xgb_tune_caret.rds")
```


```{r}
readRDS("data/xgb_tune_caret.rds")

```

We found the best nrounds is 30. 

Up to now, we have got all best parameters for xgboost model we need. And we train the final model.
```{r eval = FALSE}
params <- list(
  objective = "binary:logistic",
  eta = 0.2,
  max_depth = 7,
  colsamply_bytree = 0.8,
  min_child_weight = 10,
  subsample = 0.8
  #gamma = 10
)

set.seed(123)
xgb <- xgb.train(
    nrounds = 30,
    params = params,
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")


pred = predict(xgb, dtest)
pentathlon_nptb_test$`pred_xgb` <- pred
#model_eval(intuit75k_test, 'pred_xgb')
AUC <- tail(xgb$evaluation_log$val_auc,1)

saveRDS(pred, "data/xgb_best.rds")
AUC
```

The final auc of the best xgboost model is 0.8871 and the prediction is stored in columnï¼špred_xgb in test set.


```{r}
A <- pentathlon_nptb_total %>%
  select(c('buyer','total_os','message','endurance_os','strength_os','water_os','team_os','backcountry_os','winter_os','racquet_os'))

A <- pentathlon_nptb_total %>%
  group_by(message) %>%
  summarize(mean = sum(total_os) / sum(buyer == 'yes')) %>%
  mutate(message = as.character(message))


        #sum_ = sum(total_os),
        #no_buy = sum(buyer == 'yes'),
```

nn with size = 7, decay = 0.1

```{r}

result <- nn(
  pentathlon_nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"), 
  lev = "yes", 
  size = 7, 
  decay = 0.1, 
  seed = 1234
)
pred <- predict(result, pred_data = pentathlon_nptb_test)
pentathlon_nptb_test <- store(pentathlon_nptb_test, pred, name = 'pred_nn_final')

```

Create data

```{r}
## filter and sort the dataset
pentathlon_nptb_rep <- pentathlon_nptb %>%
  filter(representative == 1) %>%
  select(custid:representative)
register("pentathlon_nptb_rep", "pentathlon_nptb")
# dtab(pentathlon_nptb_rep, dec = 2, nr = 100) %>% render()
```


```{r}
## filter and sort the dataset
pentathlon_nptb_total <- pentathlon_nptb %>%
  filter(representative == 0) %>%
  select(custid:representative)
register("pentathlon_nptb_total", "pentathlon_nptb")
# dtab(pentathlon_nptb_total, dec = 2, nr = 100) %>% render()
```

```{r}

result <- nn(
  pentathlon_nptb_total, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "income", "education", "children", 
    "freq_endurance", "freq_strength", "freq_water", "freq_team", 
    "freq_backcountry", "freq_winter", "freq_racquet"
  ), 
  lev = "yes", 
  size = 7, 
  decay = 0.1, 
  seed = 1234
)
summary(result, prn = TRUE)

pred <- predict(result, pred_data = pentathlon_nptb_rep)
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'endurance'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_endu")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'strength'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_stren")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'water'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_water")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'team'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_team")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'backcountry'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_back")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'winter'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_winter")


pred <- predict(result, pred_data = pentathlon_nptb_rep, pred_cmd = "message = 'racquet'")
pentathlon_nptb_rep <- store(pentathlon_nptb_rep, pred, name = "pred_nn7_racquet")
```
## Q1

```{r}
pentathlon_nptb_rep <- pentathlon_nptb_rep %>%
  mutate(pred_adj = pred_nn7 / (pred_nn7 + (1-pred_nn7) * (1-0.01) / 0.01),
         pred_adj_endu =pred_nn7_endu / (pred_nn7_endu + (1-pred_nn7_endu) * (1-0.01) / 0.01),
         pred_adj_stren =pred_nn7_stren / (pred_nn7_stren + (1-pred_nn7_stren) * (1-0.01) / 0.01),
         pred_adj_water =pred_nn7_water / (pred_nn7_water + (1-pred_nn7_water) * (1-0.01) / 0.01),
         pred_adj_team =pred_nn7_team/ (pred_nn7_team + (1-pred_nn7_team) * (1-0.01) / 0.01),
         pred_adj_back =pred_nn7_back / (pred_nn7_back + (1-pred_nn7_back) * (1-0.01) / 0.01),
         pred_adj_winter =pred_nn7_winter/ (pred_nn7_winter + (1-pred_nn7_winter) * (1-0.01) / 0.01),
         pred_adj_racquet =pred_nn7_racquet / (pred_nn7_racquet + (1-pred_nn7_racquet) * (1-0.01) / 0.01),) %>%
  mutate(message_to = c('endurance','strength','water','team','backcountry','winter','racquet')[which.pmax(pred_adj_endu, pred_adj_stren, pred_adj_water, pred_adj_team, pred_adj_back,pred_adj_winter, pred_adj_racquet)])

```


## Q2 
```{r}
temp <- pentathlon_nptb_rep %>%
  group_by(message_to) %>%
  summarize(count = n(),
            percentage = count / nrow(pentathlon_nptb_rep)) %>%
  arrange(desc(percentage))
temp
```


## Q3
```{r}
expected = as.vector(A$mean)
## endurance , strength, water,team,backcountry,winter ,racquet
pentathlon_nptb_rep <- pentathlon_nptb_rep %>%
  mutate(exp_endu = pred_adj_endu * expected[1] * 0.4,
         exp_stren = pred_adj_stren * expected[2]* 0.4,
         exp_water = pred_adj_water * expected[3]* 0.4,
         exp_team = pred_adj_team * expected[4]* 0.4,
         exp_back = pred_adj_back * expected[5]* 0.4,
         exp_winter = pred_adj_winter * expected[6]* 0.4,
         exp_racquet = pred_adj_racquet * expected[7]* 0.4
         ) %>%
  mutate(message_to_exp = c('endurance','strength','water','team','backcountry','winter','racquet')[which.pmax(exp_endu, exp_stren, exp_water, exp_team, exp_back,exp_winter, exp_racquet)])

```


## Q4
```{r}

temp <- pentathlon_nptb_rep %>%
  group_by(message_to_exp) %>%
  summarize(count = n(),
            percentage = count / nrow(pentathlon_nptb_rep)) %>%
  arrange(desc(percentage))
temp

```

## Q5

```{r}
A <- A %>%
  mutate(message = as.character(message))


pentathlon_nptb_rep <- pentathlon_nptb_rep %>%
  mutate(max_pro = pmax(exp_endu,exp_stren,exp_water,exp_team,exp_back,exp_winter,exp_racquet))


mean(pentathlon_nptb_rep$max_pro)
```


## Q6

```{r}
mean(pentathlon_nptb_rep$exp_endu)
mean(pentathlon_nptb_rep$exp_stren)
mean(pentathlon_nptb_rep$exp_water)
mean(pentathlon_nptb_rep$exp_team)
mean(pentathlon_nptb_rep$exp_back)
mean(pentathlon_nptb_rep$exp_winter)
mean(pentathlon_nptb_rep$exp_racquet)
```


## Q7


```{r}
pentathlon_nptb_rep <- pentathlon_nptb_rep %>%
  left_join(A, by=c("message"="message"))


```


```{r}
pentathlon_nptb_rep <- pentathlon_nptb_rep %>%
  mutate(random = pred_adj*mean.x*0.4)

mean(pentathlon_nptb_rep$random)
```


## Q8

```{r}
(mean(pentathlon_nptb_rep$max_pro)-mean(pentathlon_nptb_rep$random)) * 5000000 


((mean(pentathlon_nptb_rep$max_pro)-mean(pentathlon_nptb_rep$random)) * 5000000) /(mean(pentathlon_nptb_rep$random)*5000000)
```


## New Proposal

* weakness 1 : The result may be incidental and not able to get updated information 

Our prediction is only based on customers' responces to last one email, the result may occur by chance and may not be able to represent the real preference of that customer. If further analytics is only based on their responces of top 2 message type, we may never get there responces to other messages. Our strategy towards them will be very biased and out of date.

* Improvement

For first two weeks, randomly send them emails of 7 kinds. During second week, we analyze their responces from first week and forecast two messages that yield the highest expected profit. For the following two weeks, those 2 departments each control 1/2 of messages. In this way, we can capture their real time preference for each month and send them targeted messages to generate more profit in the second half of that month. 

and 
