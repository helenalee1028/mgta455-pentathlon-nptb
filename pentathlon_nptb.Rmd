---
title: "Pentathlon III: Next Product to Buy Modeling"
output: html_document
---

* Team-lead gitlab id: 2724742
* Team-lead gitlab username: rsm-wel030
* Group number: 10
* Group name: Group_10
* Team member names: Wenrui Li, Shumeng Shi, Menghui Zhang

```{r r_setup, include = FALSE}
## initial settings
knitr::opts_chunk$set(
  comment = NA,
  echo = TRUE,
  error = TRUE,
  cache = FALSE,
  message = FALSE,
  dpi = 96,
  warning = FALSE
)

## width to use when printing tables etc.
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE
)

## load radiant packages if neededi
if (!exists("r_environment")) library(radiant)
```

```{r include = FALSE}
library(dplyr)
library(tidyverse)
library(caret)
library(recipes)
library(keras)
library(yardstick)
library(ggplot2)
library(gbm)
library(ModelMetrics)
```

<style>
.table {
  width: auto;
}
ul, ol {
  padding-left: 18px;
}
pre, code, pre code {
  overflow: auto;
  white-space: pre;
  word-wrap: normal;
  background-color: #ffffff;
}
</style>

## Setup

Please complete this Rmarkdown document by answering the questions in `pentathlon-nptb.pdf` on Dropbox (week8/readings/). The code block below will load the data you need. Please DO NOT change the code used to load the data. Create an HTML file with all your results and comments and push both the Rmarkdown and HTML file to GitLab when you are done. As always, all results MUST be reproducible (i.e., the TA and I must be able to recreate the HTML from the R-markdown file without changes or errors).

Good luck!

```{r}
## Loading the data from Dropbox/MGTA455-2019/data/
pentathlon_nptb <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/pentathlon_nptb.rds"))
```

### Summary of Analysis Report

1. Model selection 
* Used logistic regression, Keras Deep Learning, Neural Network, GBM and XGboost to fit training data (variable ‘training’ equals 1) and get the optimal model which yields the highest AUC score in test dataset (variable ‘training’ equals 0). 

2. Probability prediction
* After selecting the optimal model, we train the model again on the entire dataset where variable ‘representative’ equals 0. 
* Then we use the real test dataset where variable ‘representative’ equals 1 for prediction. 8 series of predictions were generated to represent customers’ purchasing probability upon receiving endurance, strength, backcountry, racquet, winter, water, team or any of the 7 categories of messages, respectively. 
* Predicted probabilities were adjusted to accommodate the actual purchasing proportion, which is around 1% on average.

3. Message Type for Email Marketing Campaign
* message type based on the highest ranked probability - For example, if the likelihood of a customer purchasing is highest when we send him a message on endurance products, then endurance message is the target message type. 
* message type based on the highest ranked expected profits - The average order size for each message type is the average total order size for customers who receive that particular message type and purchase. To get the expected profits, we multiply the predicted probability by the average order size and profit rate of 40%. The target message category is the one with the highest expected profit for each customer.

4. Effectiveness Measurement of NPTB Strategy
* Given a typical promotional email blast to 5,000,000 customers, we calculated the improvement of expected profits with targeting procedure over with random assignment, both in terms of absolute euros and in percentage.

### Model Selection

#### Evaluation Metrics

First we define a function to evaluate models by AUC and accuracy.

```{r}
cm <- function(dat, vars){
  
  cm_df <- as.data.frame(matrix(NA, ncol = 3, nrow = length(vars)))
  colnames(cm_df) <- c("var", "auc", "accuracy")
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    probs <- pull(dat, !!var)
    resp <- pull(dat, "buyer")
    
    predict <- ifelse(pull(dat, !!var) > 0.5, "TRUE", "FALSE") # predict whether a customer will buy 
    
    accuracy <- (sum(resp == "yes" & predict == "TRUE") + sum(resp == "no" & predict == "FALSE"))/nrow(dat)
    
    auc <- ModelMetrics::auc(ifelse(resp=="yes",1,0), probs)
    
    cm_vec <- c(var, auc, accuracy)
    cm_df[i,] <- cm_vec
  }
  return(cm_df)
}
```

### Logistic Regression

In logistic regression, we included all message related interaction terms to incorporate the fact that different customers act differently even to the same type of messages. Also, we included other interaction terms including income and frequency. 

```{r}
# split into training, testing and representative datasets, and log-transform income
nptb <- pentathlon_nptb %>% 
  filter(!is.na(training)) %>% 
  mutate(income_ln = log(income+1)) # in case of zero incomes

nptb_train <- nptb %>% 
  filter(training == 1)

nptb_test <- nptb %>% 
  filter(training == 0)

nptb_repr <- pentathlon_nptb %>% 
  filter(representative == "1") %>% 
  mutate(income_ln = log(income+1))

# run logistic regression with message related interaction terms
result <- logistic(
  nptb_train, 
  rvar = "buyer", 
  evar = c(
    "message", "age", "gender", "education", "children", "freq_endurance", 
    "freq_strength", "freq_water", "freq_team", "freq_backcountry", 
    "freq_winter", "freq_racquet", "income_ln"
  ), 
  lev = "yes", 
  int = c(
    "message:age", "message:gender", 
    "message:education", 
    "message:children", "message:freq_endurance", 
    "message:freq_strength", 
    "message:freq_water", 
    "message:freq_team", 
    "message:freq_backcountry", 
    "message:freq_winter", 
    "message:freq_racquet", 
    "message:income_ln", 
    "freq_endurance:income_ln", 
    "freq_strength:income_ln", 
    "freq_water:income_ln", 
    "freq_team:income_ln", 
    "freq_backcountry:income_ln", 
    "freq_winter:income_ln", 
    "freq_racquet:income_ln"
  )
)
summary(result)

# prediction on 30k test data
logit_pred <- predict(result, pred_data = nptb_test)
```

#### Neural Network 

Also, we used single layer neural network to capture the interaction effects. In this step, we looped over node size from 1 to 10 and decay range from 0.1 to 0.8. Then we used the optimal model to run predictions on test dataset. 

```{r}

```

#### Keras MLP
```{r}

```

